{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modelTest2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEeXfOe7PKVu",
        "colab_type": "text"
      },
      "source": [
        "# READ - 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UMSsjCK6vW3",
        "colab_type": "text"
      },
      "source": [
        "**This is just the testing part of the model that we created in Experimental.ipynb**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEmEv0Co7GAI",
        "colab_type": "text"
      },
      "source": [
        "we can avoid using tensorflow instead we should prefer just using keras , if you want to deploy your model to online platform such as heroku. Because tensorflow uses a lot of memory and heroku one dyno (free ) version provides only 512 mb of RAM. Believe me it was such a pain in the ... :-) . Tensorflow 2.2.0 has installation size of approx 459 mb which almost takes 95% of your space. Heroku has this compressing facility which creates slugs after compression even though my slug size was 650 mb due to this the build (deployment) failed. \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "SOLUTION :- what i did is degraded my tensorflow version to 2.0.1 in      requirements.txt even though my backend was tf2.2.0 it worked.\n",
        "\n",
        "i have attached 'requirements.txt' in [GIT REPO](https://github.com/cd-x/test-pdd)  there you can see it is quite a big list of requirements. By the way you create reuirements.txt just by using command \n",
        "\n",
        "```\n",
        "$pip install freeze\n",
        "$pip freeze > requirements.txt\n",
        "```\n",
        "we'll discuss more on deployment in another notebook\n",
        "\n",
        "# Code Explanation\n",
        "\n",
        "TO load the trained model saved as 'acc9275own.h5' (h5 is the extension for model weights ) we need to import load_model from keras.models \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3GPIR4tJvsd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l1KLq6_AL5b",
        "colab_type": "text"
      },
      "source": [
        "An interesting thing i found during loading the model is i was getting error as FILE NOT FOUND: no model found at /path/to/model { save_model.pb | save_model.txt } \n",
        "\n",
        "it was because as i have explained in experimental.ipynb that i was using **tensorflow_hub** to import pretrained feature vector but it doesn't get combined with the rest of the layers so the model didn't build succesfully. \n",
        "\n",
        "what colab does is it hides all the tedious task from you and create tfhub_modules in /tmp directory of your mounted drive\n",
        "\n",
        "To reload model trained through **tensorflow_hub** what you need to do is \n",
        "save the model as follows\n",
        "\n",
        "```\n",
        "import time\n",
        "t = time.time()\n",
        "\n",
        "export_path = \"/tmp/saved_models/{}\".format(int(t))\n",
        "tf.keras.models.save_model(model,export_path,save_format='h5')\n",
        "```\n",
        "It stores the model in /tmp/saved_model/ named as timestamp file and the directory will look like this\n",
        "\n",
        "---tmp/\n",
        "\n",
        "-----/saved_models\n",
        "\n",
        "--------1234567\n",
        "\n",
        "-----/tfhub_modules\n",
        "\n",
        "-------/assets\n",
        "\n",
        "-------/variables\n",
        "\n",
        "----------variable index\n",
        "\n",
        "----------variable.0000 - 0001\n",
        "\n",
        "-------saved_model.pb\n",
        "\n",
        "\n",
        "\n",
        "to reload the model the code will be as follows \n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "model = tf.keras.models.load_model(export_path, custom_objects={'KerasLayer':hub.KerasLayer})\n",
        "```\n",
        "NOTE 1:- even though training through tf_hub was easy with same accuracy as of now , it lead me to failure in deployment \n",
        "\n",
        "NOTE 2:- you can create above mentioned directory in another way by saving the model as **\"model.h5py\"** file .\n",
        "\n",
        "**Remember i discarded the above approach for the sake of heroku**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwv_UCyVKHE0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=load_model('acc9275own.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxxlTGVkJEqS",
        "colab_type": "text"
      },
      "source": [
        "# LOADING categories.json\n",
        "\n",
        "you can find the file in git repo it has all the categories mentioned in json dictionary so i am getting all the values of the dictionary and storing as list because dictioanry is not iterable in python. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZio_encK83r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "fad4bee8-7633-409a-ca09-6a63987df23a"
      },
      "source": [
        "import json\n",
        "\n",
        "with open('categories.json', 'r') as f:\n",
        "    cat_to_name = json.load(f)\n",
        "    classes = list(cat_to_name.values())\n",
        "    \n",
        "print (classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Apple___Apple_scab', 'Apple___Black_rot', 'Apple___Cedar_apple_rust', 'Apple___healthy', 'Blueberry___healthy', 'Cherry_(including_sour)___Powdery_mildew', 'Cherry_(including_sour)___healthy', 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot', 'Corn_(maize)___Common_rust_', 'Corn_(maize)___Northern_Leaf_Blight', 'Corn_(maize)___healthy', 'Grape___Black_rot', 'Grape___Esca_(Black_Measles)', 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)', 'Grape___healthy', 'Orange___Haunglongbing_(Citrus_greening)', 'Peach___Bacterial_spot', 'Peach___healthy', 'Pepper,_bell___Bacterial_spot', 'Pepper,_bell___healthy', 'Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy', 'Raspberry___healthy', 'Soybean___healthy', 'Squash___Powdery_mildew', 'Strawberry___Leaf_scorch', 'Strawberry___healthy', 'Tomato___Bacterial_spot', 'Tomato___Early_blight', 'Tomato___Late_blight', 'Tomato___Leaf_Mold', 'Tomato___Septoria_leaf_spot', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Target_Spot', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus', 'Tomato___Tomato_mosaic_virus', 'Tomato___healthy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FrEbCkKKBu5",
        "colab_type": "text"
      },
      "source": [
        "This image size  is fixed because our model has given input of 224 X 224 changing configuration will lead in model error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMK1j9zlLmPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMAGE_SIZE=(224,224)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKS_k_7tKbFW",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "***load_image():*** method preprocess the given image using opencv_python (compuetr vision loads an image and resize in the input format of our sequential model)\n",
        "\n",
        "***predict():*** method passes input tensor (created by numpy) to the model and will call predict method of keras.models. predict returns confidence of each 38 classes. Using numpy's argmax will return the index of max value comes first in the array for e.g. arr = 5 1 2 3 4 5 then argmax(arr) = 0.\n",
        "\n",
        "After getting index we'll return category name present in the classes variable ,that we obtained previously from json\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzTJ0OitLF5C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import OpenCV\n",
        "import cv2\n",
        "\n",
        "# Utility\n",
        "import itertools\n",
        "import random\n",
        "from collections import Counter\n",
        "from glob import iglob\n",
        "\n",
        "def load_image(filename):\n",
        "    img = cv2.imread(filename)\n",
        "    #img = cv2.imread(os.path.join(image_dir, filename)) #<-- use in case of test through existing validation dataset\n",
        "    img = cv2.resize(img, (IMAGE_SIZE[0], IMAGE_SIZE[1]) )\n",
        "    img = img /255\n",
        "    \n",
        "    return img\n",
        "\n",
        "\n",
        "def predict(image):\n",
        "    probabilities = model.predict(np.asarray([image]))[0]\n",
        "    class_idx = np.argmax(probabilities)\n",
        "    \n",
        "    return {classes[class_idx]: probabilities[class_idx]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQO0MqtSONiS",
        "colab_type": "text"
      },
      "source": [
        "# Upload image from anywhere and test it\n",
        "\n",
        "having fun colab features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1oCH6Y_LP6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = fn\n",
        "  img= load_image(path)\n",
        "  result= predict(img)\n",
        "  print(result)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}